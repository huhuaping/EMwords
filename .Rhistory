# delete rows with less words
filter(str_detect(heading, "^#") | tot_words > 4)
(file_out <- glue("data/hansen/sentences-dt{name_use}.rds"))
write_rds(dt_simple, file_out)
file_path <- files_all[4]
name_use <- str_extract(file_path, "(?<=chpt)(.+)(?=-chn)")
dt_clean <- read_rds(file_path) %>%
filter(gotrans) %>%
select(text_nohash, chn, heading) %>%
mutate(chn = unlist(chn)) %>%
mutate(text_nohash = str_trim(text_nohash)) %>%
filter(!is.na(text_nohash) | text_nohash!="") %>%
# detect total numbers of sentence
mutate(
tot_sentence = str_count(text_nohash, "\\. ") + 1,
tot_sentence_chn = str_count(chn, "。")
) %>%
mutate(tot_sentence_chn = ifelse(
str_detect(heading, "#")&tot_sentence_chn==0,
tot_sentence_chn + 1,
tot_sentence_chn
)
) %>%
# extract the first sentence
mutate(
eng_first = str_extract(text_nohash, "(^.*?)(?=\\. [A-Z])"),
chn_first = str_extract(chn, "(^.*?)(?=。)")
) %>%
# handle lines with only sentence.
mutate(
eng_first = ifelse(
is.na(eng_first) &!is.na(text_nohash) & tot_sentence==1,
text_nohash,
eng_first
),
chn_first = ifelse(
is.na(chn_first) &!is.na(chn) & tot_sentence==1,
chn,
chn_first
)
) %>%
# extract the last sentence
mutate(
#eng_last = str_extract(text_nohash, "(?<=\\. )([A-Z].*?)\\.$"),
#chn_last = str_extract(chn, "(?<=。)(.*?)(?=。$)")
eng_last = ifelse(
tot_sentence > 1,
#strex::str_after_last(text_nohash, "[a-z]{2,50}\\. "),
strex::str_after_nth(text_nohash, "\\. ", n = tot_sentence -1),
NA
#str_extract(text_nohash, "[\\w\\s]+.$")
#sub('.*\\. ', '', text_nohash)
),
tot_sentence_chn = replace_na(tot_sentence_chn, 0),
chn_last =ifelse(
tot_sentence_chn >1 ,
strex::str_after_nth(chn, "。", n = tot_sentence_chn -1),
NA
#str_extract(chn, "[\\w\\s]+。$")
)
) %>%
# tidy the last sentence
mutate(
eng_last = str_replace(eng_last, ".$", ""),
chn_last = str_replace(chn_last, "。$", "")
) %>%
# remove some lines
filter(!str_detect(text_nohash, "^\\*")) %>%
filter(!str_detect(text_nohash, "^\\$")) %>%
filter(!str_detect(text_nohash, "^\\("))
# ====construct tsv format====
## get the index of the Exercises
index_ex <- which(dt_clean$text_nohash=="Exercises") -1
if (length(index_ex) ==0) {
index_ex <- nrow(dt_clean)
}
## remove the meaningless lines
ptn <- c("with", "or", "Thus", "and", "where",
"Also", "Hence")
dt_tsv <- dt_clean %>%
# remove lines of "Exercises"
.[1:index_ex, ] %>%
# handle no match pairs
mutate(
eng_first = ifelse(eng_first %in% ptn, NA, eng_first),
eng_last = ifelse(eng_last %in% ptn, NA, eng_last)
) %>%
mutate(
st_first = str_c(eng_first, chn_first, sep=" hhp "),
st_last = str_c(eng_last, chn_last, sep=" hhp")
) %>%
filter(!is.na(st_first)|!is.na(st_last)) %>%
add_column(index = 1:nrow(.) ) %>%
select(index, heading, st_first, st_last) %>%
mutate(tsv = map2(.x = st_first, .y = st_last,
.f = function(x, y)tibble(st =c(x, y))
)
) %>%
select(index, heading, tsv) %>%
unnest(cols = tsv) %>%
filter(!is.na(st)) %>%
# tidy and filter again
mutate(n_sentence = str_count(st, "。")+1) %>%
filter(n_sentence ==1) %>%
filter(st!=" hhp ") %>%
separate(st, into = c("eng", "chn"), sep = "hhp") %>%
select(heading, eng, chn)
#==== simplify sentences====
dt_simple <- dt_tsv %>%
# trim both side
mutate_all(., str_trim, side = "both") %>%
# remove the tail dot
mutate(eng = str_replace(eng, "\\.$|\\*$", "")) %>%
mutate(chn = str_replace(chn, "\\.$|\\*$", "")) %>%
# delete rows containing dollar sign or url link
filter(!str_detect(eng, "\\$|\\\\href")) %>%
# delete rows start with slash sign
filter(!str_detect(eng, "^\\\\")) %>%
# delete rows with items style
filter(!str_detect(eng, "^\\d{1,2}\\.")) %>%
# count words in the sentence
mutate(tot_words = str_count(eng, "\\w+")) %>%
# delete rows with less words
filter(str_detect(heading, "^#") | tot_words > 4)
files_all <- dir("text/hansen",full.names = T) # all files
files_all <- dir("text/hansen",full.names = T) # all files
tot <- length(files_all)
files_all <- dir("text/hansen",full.names = T) # all files
tot <- length(files_all)
for (i in 1:tot){
#====read and clean data set====
file_path <- files_all[i]
name_use <- str_extract(file_path, "(?<=chpt)(.+)(?=-chn)")
dt_clean <- read_rds(file_path) %>%
filter(gotrans) %>%
select(text_nohash, chn, heading) %>%
mutate(chn = unlist(chn)) %>%
mutate(text_nohash = str_trim(text_nohash)) %>%
filter(!is.na(text_nohash) | text_nohash!="") %>%
# detect total numbers of sentence
mutate(
tot_sentence = str_count(text_nohash, "\\. ") + 1,
tot_sentence_chn = str_count(chn, "。")
) %>%
mutate(tot_sentence_chn = ifelse(
str_detect(heading, "#")&tot_sentence_chn==0,
tot_sentence_chn + 1,
tot_sentence_chn
)
) %>%
# extract the first sentence
mutate(
eng_first = str_extract(text_nohash, "(^.*?)(?=\\. [A-Z])"),
chn_first = str_extract(chn, "(^.*?)(?=。)")
) %>%
# handle lines with only sentence.
mutate(
eng_first = ifelse(
is.na(eng_first) &!is.na(text_nohash) & tot_sentence==1,
text_nohash,
eng_first
),
chn_first = ifelse(
is.na(chn_first) &!is.na(chn) & tot_sentence==1,
chn,
chn_first
)
) %>%
# extract the last sentence
mutate(
#eng_last = str_extract(text_nohash, "(?<=\\. )([A-Z].*?)\\.$"),
#chn_last = str_extract(chn, "(?<=。)(.*?)(?=。$)")
eng_last = ifelse(
tot_sentence > 1,
#strex::str_after_last(text_nohash, "[a-z]{2,50}\\. "),
strex::str_after_nth(text_nohash, "\\. ", n = tot_sentence -1),
NA
#str_extract(text_nohash, "[\\w\\s]+.$")
#sub('.*\\. ', '', text_nohash)
),
tot_sentence_chn = replace_na(tot_sentence_chn, 0),
chn_last =ifelse(
tot_sentence_chn >1 ,
strex::str_after_nth(chn, "。", n = tot_sentence_chn -1),
NA
#str_extract(chn, "[\\w\\s]+。$")
)
) %>%
# tidy the last sentence
mutate(
eng_last = str_replace(eng_last, ".$", ""),
chn_last = str_replace(chn_last, "。$", "")
) %>%
# remove some lines
filter(!str_detect(text_nohash, "^\\*")) %>%
filter(!str_detect(text_nohash, "^\\$")) %>%
filter(!str_detect(text_nohash, "^\\("))
# ====construct tsv format====
## get the index of the Exercises
index_ex <- which(dt_clean$text_nohash=="Exercises") -1
if (length(index_ex) ==0) {
index_ex <- nrow(dt_clean)
}
## remove the meaningless lines
ptn <- c("with", "or", "Thus", "and", "where",
"Also", "Hence")
dt_tsv <- dt_clean %>%
# remove lines of "Exercises"
.[1:index_ex, ] %>%
# handle no match pairs
mutate(
eng_first = ifelse(eng_first %in% ptn, NA, eng_first),
eng_last = ifelse(eng_last %in% ptn, NA, eng_last)
) %>%
mutate(
st_first = str_c(eng_first, chn_first, sep=" hhp "),
st_last = str_c(eng_last, chn_last, sep=" hhp")
) %>%
filter(!is.na(st_first)|!is.na(st_last)) %>%
add_column(index = 1:nrow(.) ) %>%
select(index, heading, st_first, st_last) %>%
mutate(tsv = map2(.x = st_first, .y = st_last,
.f = function(x, y)tibble(st =c(x, y))
)
) %>%
select(index, heading, tsv) %>%
unnest(cols = tsv) %>%
filter(!is.na(st)) %>%
# tidy and filter again
mutate(n_sentence = str_count(st, "。")+1) %>%
filter(n_sentence ==1) %>%
filter(st!=" hhp ") %>%
separate(st, into = c("eng", "chn"), sep = "hhp") %>%
select(heading, eng, chn)
#==== simplify sentences====
dt_simple <- dt_tsv %>%
# trim both side
mutate_all(., str_trim, side = "both") %>%
# remove the tail dot
mutate(eng = str_replace(eng, "\\.$|\\*$", "")) %>%
mutate(chn = str_replace(chn, "\\.$|\\*$", "")) %>%
# delete rows containing dollar sign or url link
filter(!str_detect(eng, "\\$|\\\\href")) %>%
# delete rows start with slash sign
filter(!str_detect(eng, "^\\\\")) %>%
# delete rows with items style
filter(!str_detect(eng, "^\\d{1,2}\\.")) %>%
# count words in the sentence
mutate(tot_words = str_count(eng, "\\w+")) %>%
# delete rows with less words
filter(str_detect(heading, "^#") | tot_words > 4)
#====write out====
(file_out <- glue("data/hansen/sentences-dt{name_use}.rds"))
write_rds(dt_simple, file_out)
cat(glue("the {i}th of total {tot} files"))
}
cat(glue("the {i}th of total {tot} files /n"))
cat(glue("the {i}th of total {tot} files /n"))
files_all <- dir("data/hansen",full.names = T) # all files
tot <- length(files_all)
dt_read <- tibble(path = files_all)
View(dt_read)
dt_read <- tibble(path = files_all) %>%
mutate(chpt = str_extract(path, "(?<=sentences).+(?=\\.rds)"))
dt_read <- tibble(path = files_all) %>%
mutate(chpt = str_extract(path, "(?<=sentences-).+(?=\\.rds)"))
dt_read <- tibble(path = files_all) %>%
mutate(chpt = str_extract(path, "(?<=sentences-).+(?=\\.rds)")) %>%
mutate(dt = map(path, read_rds))
View(dt_read[[3]][[1]])
dt_read <- tibble(path = files_all) %>%
mutate(chpt = str_extract(path, "(?<=sentences-).+(?=\\.rds)")) %>%
mutate(dt = map(path, read_rds)) %>%
unnest(cols = "dt")
View(dt_read)
dt_read <- tibble(path = files_all) %>%
mutate(chpt = str_extract(path, "(?<=sentences-).+(?=\\.rds)")) %>%
mutate(dt = map(path, read_rds)) %>%
unnest(cols = "dt") %>%
select(-path)
renv::install("janitor")
#renv::install("janitor")
require(janitor)
janitor::get_dupes(select(dt_read, eng))
dt_clean <- dt_read %>%
# delete rows with duplicated sentence
distinct(., eng, .keep_all = TRUE)
View(dt_clean)
tbl_check <- janitor::get_dupes(select(dt_read, eng))
View(tbl_check)
write_tsv(select(dt_clean, eng, chn), file = file_out, col_names = FALSE)
file_out <- "data/tsv_hansen.tsv"
write_tsv(select(dt_clean, eng, chn), file = file_out, col_names = FALSE)
file_out <- "data/tsv_hansen_hand.tsv"
write_tsv(select(dt_clean, eng, chn), file = file_out, col_names = FALSE)
`chpt04-lsr-chn` <- readRDS("D:/github/EMwords/text/hansen/chpt04-lsr-chn.rds")
View(`chpt04-lsr-chn`)
renv::status()
renv::snapshot()
file_out <- "tsv/hansen.tsv"
write_tsv(select(dt_clean, eng, chn), file = file_out, col_names = FALSE)
file_out <- "tsv/hansen_hand.tsv"
write_tsv(select(dt_clean, eng, chn), file = file_out, col_names = FALSE)
# ==== pkgs ====
require(knitr)
require(tidyverse)
require(stringr)
require(mgsub)
require(purrr)
require(here)
#renv::install("ropensci/googleLanguageR")
require(googleLanguageR)
# ==== read file ====
file_path <- here("text/index/abbrev-Cameron-Stata-a.qmd")
#file_path <- chpt_path
tbl_tex <- readLines(file_path) %>%
as_tibble() %>%
rename("text" = "value") %>%
add_column(row = 1:nrow(.),
.before = "text")
#file_path <- chpt_path
tbl_tex <- readLines(file_path) %>%
as_tibble() %>%
rename("text" = "value") %>%
add_column(row = 1:nrow(.),
.before = "text")
View(tbl_tex)
tbl_dollar <- tbl_tex %>%
mutate(
double_dollar = str_detect(text, "^\\$\\$$"),
index_raw = row_number(row),
index_detect = ifelse(
double_dollar==TRUE,
index_raw,NA)
)
# identify position
lines <- tbl_dollar %>%
filter(!is.na(index_detect)) %>%
pull(index_detect)
if (length(lines) >0) {
# case exist math environment
# assumes existing perfect paired double dollar symbols
row_tar <- seq_len(length(lines))%%2  # row indicator
lines_star <- lines[row_tar==1]   # start row (odd)
lines_end <- lines[row_tar==0]   # end row (enven)
lines_tar <- NULL
for (i in 1:length(lines_star)){
lines_tem <- lines_star[i]:lines_end[i]
lines_tar <- c(lines_tar, lines_tem)
}
tbl_dollar  <- tbl_dollar %>%
# tag lines within double dollar pairs
mutate(
dollars = ifelse(
index_raw %in% lines_tar,
TRUE, FALSE)
)
} else {
# case no math environment
tbl_dollar  <- tbl_dollar %>%
# tag lines within double dollar pairs
mutate( dollars = FALSE )
}
# tag it
tbl_work <- tbl_dollar  %>%
mutate(
tabs = str_detect(text, "^\\|.*?\\|$")) %>%
# tag lines of markdown image
mutate(
img = str_detect(text, "^\\!\\[\\]\\(images")
) %>%
# tag empty lines
mutate(
empty = ifelse(text=="", TRUE, FALSE)
) %>%
# tag lines need to translate
mutate(
gotrans = ifelse(
dollars+tabs+img+empty==0,
TRUE, FALSE)
) %>%
# tag heading
mutate(
heading = str_extract(text, "^#{1,3}"),
text_nohash = str_replace_all(text, "(^#{1,3})", "")
)
trans_mathParagraph <- function(text, auth) {
# prepare
library(googleLanguageR)
library(mgsub)
gl_auth(auth)
# handle dollar units "\\$ 15.6"
## first substitute the unit dollar,
## then we have to change it back.
text <- mgsub::mgsub(
string =  text,
pattern = "\\\\\\$",
replacement = "us_dollar"
)
# now we split whole paragraph
split_raw <- strsplit(text,
split = "(?=[\\$])",
perl = TRUE) %>%
unlist()
lines <- grep("\\$", split_raw)
split_handle <- split_raw
# identify position
row_tar <- seq_len(length(lines))%%2  # row indicator
lines_star <- lines[row_tar==1]   # start row (odd)
lines_end <- lines[row_tar==0]   # end row (enven)
# loop substitute
# i <- 1
lines_math <- NULL
lines_handle <- NULL
for (i in 1:length(lines_star)){
# text of raw math
lines_math[i] <- split_handle[lines_star[i]+1]
# replace
lines_eq <- paste0("matheq", i)
split_handle[lines_star[i]+1] <- lines_eq
# text of handle math
lines_handle[i] <- lines_eq
}
# paste as paragraphs
paragraph_handle <- paste0(split_handle, collapse = "")
# now translate it
trans_result <- googleLanguageR::gl_translate(
t_string = paragraph_handle,
target = "zh-CN")$translatedText
Sys.sleep(0.2)
# then re-split the paragraph
split_trans <- strsplit(trans_result,
split = "(?=[\\$])",
perl = TRUE) %>%
unlist()
# bug fix: inline math with chn characters, such as "$matheq4 之间$"
##  just simplify it by remove cjk characters
split_trans <- str_replace_all(
split_trans,
"(?<=^matheq\\d{1,3})(.*)",
""
)
# match math after translation
## note: math eq order may not be the same sequence as it before.
## here is the math pairs
tbl_math <- tibble(
math_raw = lines_math,
math_handle = lines_handle)
## now match and replace
tbl_result <- tibble(chn = split_trans) %>%
left_join(., tbl_math,
by = c("chn"="math_handle")) %>%
mutate(
chn_final = ifelse(
str_detect(chn, "^matheq\\d{1,2}"),
math_raw, chn))
# get the paragraph
paragraph_final <- paste0(tbl_result$chn_final, collapse = "")
# now substitute the dollar unit
paragraph_final <- mgsub::mgsub(
string =  paragraph_final,
pattern = "us_dollar",
replacement = "\\\\\\$"
)
return(paragraph_final)
}
n_pars <- sum(tbl_work$gotrans)
auth_json <- "C:/Users/huhua/json/googleLanguageR.json"
n_pars
View(tbl_work)
## !important! run only once!
## Google translate will charge your fees!!
tbl_trans <- tbl_work %>%
#head(100) %>%
mutate(
chn = map2(
.x = text_nohash, .y = gotrans,
.f = function(x,y){
if(y==TRUE) {
out <- trans_mathParagraph( #custom function
text = x,
auth = auth_json)
} else{
out <- ""
}
return(out)
}
)
)
auth_json <- "C:/Users/huhua/json/googleLanguageR.json"
auth_json
n_pars <- sum(tbl_work$gotrans)
auth_json <- "C:/Users/huhua/json/googleLanguageR.json"
## !important! run only once!
## Google translate will charge your fees!!
tbl_trans <- tbl_work %>%
#head(100) %>%
mutate(
chn = map2(
.x = text_nohash, .y = gotrans,
.f = function(x,y){
if(y==TRUE) {
out <- trans_mathParagraph( #custom function
text = x,
auth = auth_json)
} else{
out <- ""
}
return(out)
}
)
)
